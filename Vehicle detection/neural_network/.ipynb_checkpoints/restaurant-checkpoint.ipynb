{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alt</th>\n",
       "      <th>Bar</th>\n",
       "      <th>Fri</th>\n",
       "      <th>Hyn</th>\n",
       "      <th>Pat</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Res</th>\n",
       "      <th>Type</th>\n",
       "      <th>Est</th>\n",
       "      <th>WillWait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Some</td>\n",
       "      <td>$$$</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>French</td>\n",
       "      <td>0–10</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Full</td>\n",
       "      <td>$</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Thai</td>\n",
       "      <td>30–60</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Some</td>\n",
       "      <td>$</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Burger</td>\n",
       "      <td>0–10</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Full</td>\n",
       "      <td>$</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Thai</td>\n",
       "      <td>10–30</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Full</td>\n",
       "      <td>$$$</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>French</td>\n",
       "      <td>&gt;60</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alt  Bar  Fri  Hyn   Pat Price Rain  Res    Type    Est WillWait\n",
       "0  Yes   No   No  Yes  Some   $$$   No  Yes  French   0–10      Yes\n",
       "1  Yes   No   No  Yes  Full     $   No   No    Thai  30–60       No\n",
       "2   No  Yes   No   No  Some     $   No   No  Burger   0–10      Yes\n",
       "3  Yes   No  Yes  Yes  Full     $  Yes   No    Thai  10–30      Yes\n",
       "4  Yes   No  Yes   No  Full   $$$   No  Yes  French    >60       No"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = \"./restaurant.csv\"\n",
    "\n",
    "# comma delimited is the default\n",
    "data = pd.read_csv(input_file, header = 0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alt</th>\n",
       "      <th>Bar</th>\n",
       "      <th>Fri</th>\n",
       "      <th>Hyn</th>\n",
       "      <th>Pat</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Res</th>\n",
       "      <th>Type</th>\n",
       "      <th>Est</th>\n",
       "      <th>WillWait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Alt  Bar  Fri  Hyn  Pat  Price  Rain  Res  Type  Est  WillWait\n",
       "0     1    0    0    1    2      2     0    1     1    0         1\n",
       "1     1    0    0    1    0      0     0    0     3    2         0\n",
       "2     0    1    0    0    2      0     0    0     0    0         1\n",
       "3     1    0    1    1    0      0     1    0     3    1         1\n",
       "4     1    0    1    0    0      2     0    1     1    3         0\n",
       "5     0    1    0    1    2      1     1    1     2    0         1\n",
       "6     0    1    0    0    1      0     1    0     0    0         0\n",
       "7     0    0    0    1    2      1     1    1     3    0         1\n",
       "8     0    1    1    0    0      0     1    0     0    3         0\n",
       "9     1    1    1    1    0      2     0    1     2    1         0\n",
       "10    0    0    0    0    1      0     0    0     3    0         0\n",
       "11    1    1    1    1    0      0     0    0     0    2         1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data_1 = data.apply(le.fit_transform)\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='sgd', \n",
    "                    alpha=1e-5, \n",
    "                    hidden_layer_sizes=(2,2), \n",
    "                    random_state=1, \n",
    "                    verbose=True, \n",
    "                    max_iter=1400) #try 1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.84056837\n",
      "Iteration 2, loss = 0.83986815\n",
      "Iteration 3, loss = 0.83886979\n",
      "Iteration 4, loss = 0.83760351\n",
      "Iteration 5, loss = 0.83609653\n",
      "Iteration 6, loss = 0.83437345\n",
      "Iteration 7, loss = 0.83245643\n",
      "Iteration 8, loss = 0.83036543\n",
      "Iteration 9, loss = 0.82811846\n",
      "Iteration 10, loss = 0.82573173\n",
      "Iteration 11, loss = 0.82321985\n",
      "Iteration 12, loss = 0.82059594\n",
      "Iteration 13, loss = 0.81787183\n",
      "Iteration 14, loss = 0.81505818\n",
      "Iteration 15, loss = 0.81216455\n",
      "Iteration 16, loss = 0.80919956\n",
      "Iteration 17, loss = 0.80617096\n",
      "Iteration 18, loss = 0.80308571\n",
      "Iteration 19, loss = 0.79995010\n",
      "Iteration 20, loss = 0.79676977\n",
      "Iteration 21, loss = 0.79354981\n",
      "Iteration 22, loss = 0.79029480\n",
      "Iteration 23, loss = 0.78700890\n",
      "Iteration 24, loss = 0.78369583\n",
      "Iteration 25, loss = 0.78035897\n",
      "Iteration 26, loss = 0.77700141\n",
      "Iteration 27, loss = 0.77362591\n",
      "Iteration 28, loss = 0.77023503\n",
      "Iteration 29, loss = 0.76683108\n",
      "Iteration 30, loss = 0.76341618\n",
      "Iteration 31, loss = 0.76013162\n",
      "Iteration 32, loss = 0.75696294\n",
      "Iteration 33, loss = 0.75380971\n",
      "Iteration 34, loss = 0.75067249\n",
      "Iteration 35, loss = 0.74755179\n",
      "Iteration 36, loss = 0.74444813\n",
      "Iteration 37, loss = 0.74149838\n",
      "Iteration 38, loss = 0.73861509\n",
      "Iteration 39, loss = 0.73576179\n",
      "Iteration 40, loss = 0.73293803\n",
      "Iteration 41, loss = 0.73014343\n",
      "Iteration 42, loss = 0.72737766\n",
      "Iteration 43, loss = 0.72464046\n",
      "Iteration 44, loss = 0.72193161\n",
      "Iteration 45, loss = 0.71925094\n",
      "Iteration 46, loss = 0.71659833\n",
      "Iteration 47, loss = 0.71397369\n",
      "Iteration 48, loss = 0.71137696\n",
      "Iteration 49, loss = 0.70880812\n",
      "Iteration 50, loss = 0.70626717\n",
      "Iteration 51, loss = 0.70375414\n",
      "Iteration 52, loss = 0.70126905\n",
      "Iteration 53, loss = 0.69881199\n",
      "Iteration 54, loss = 0.69638301\n",
      "Iteration 55, loss = 0.69398221\n",
      "Iteration 56, loss = 0.69160968\n",
      "Iteration 57, loss = 0.68926551\n",
      "Iteration 58, loss = 0.68694983\n",
      "Iteration 59, loss = 0.68466273\n",
      "Iteration 60, loss = 0.68240431\n",
      "Iteration 61, loss = 0.68017469\n",
      "Iteration 62, loss = 0.67797397\n",
      "Iteration 63, loss = 0.67580223\n",
      "Iteration 64, loss = 0.67365957\n",
      "Iteration 65, loss = 0.67154608\n",
      "Iteration 66, loss = 0.66946181\n",
      "Iteration 67, loss = 0.66740683\n",
      "Iteration 68, loss = 0.66538118\n",
      "Iteration 69, loss = 0.66338490\n",
      "Iteration 70, loss = 0.66141801\n",
      "Iteration 71, loss = 0.65948050\n",
      "Iteration 72, loss = 0.65757238\n",
      "Iteration 73, loss = 0.65569360\n",
      "Iteration 74, loss = 0.65384412\n",
      "Iteration 75, loss = 0.65202388\n",
      "Iteration 76, loss = 0.65023280\n",
      "Iteration 77, loss = 0.64847077\n",
      "Iteration 78, loss = 0.64673769\n",
      "Iteration 79, loss = 0.64503341\n",
      "Iteration 80, loss = 0.64335777\n",
      "Iteration 81, loss = 0.64171062\n",
      "Iteration 82, loss = 0.64009175\n",
      "Iteration 83, loss = 0.63850096\n",
      "Iteration 84, loss = 0.63693803\n",
      "Iteration 85, loss = 0.63540270\n",
      "Iteration 86, loss = 0.63389473\n",
      "Iteration 87, loss = 0.63241384\n",
      "Iteration 88, loss = 0.63095973\n",
      "Iteration 89, loss = 0.62953211\n",
      "Iteration 90, loss = 0.62813065\n",
      "Iteration 91, loss = 0.62675503\n",
      "Iteration 92, loss = 0.62540489\n",
      "Iteration 93, loss = 0.62407989\n",
      "Iteration 94, loss = 0.62277965\n",
      "Iteration 95, loss = 0.62150380\n",
      "Iteration 96, loss = 0.62025195\n",
      "Iteration 97, loss = 0.61902370\n",
      "Iteration 98, loss = 0.61781865\n",
      "Iteration 99, loss = 0.61663640\n",
      "Iteration 100, loss = 0.61547652\n",
      "Iteration 101, loss = 0.61433860\n",
      "Iteration 102, loss = 0.61322220\n",
      "Iteration 103, loss = 0.61212690\n",
      "Iteration 104, loss = 0.61105227\n",
      "Iteration 105, loss = 0.60999788\n",
      "Iteration 106, loss = 0.60896328\n",
      "Iteration 107, loss = 0.60794804\n",
      "Iteration 108, loss = 0.60706279\n",
      "Iteration 109, loss = 0.60622911\n",
      "Iteration 110, loss = 0.60540623\n",
      "Iteration 111, loss = 0.60459335\n",
      "Iteration 112, loss = 0.60378977\n",
      "Iteration 113, loss = 0.60299491\n",
      "Iteration 114, loss = 0.60220824\n",
      "Iteration 115, loss = 0.60142932\n",
      "Iteration 116, loss = 0.60065773\n",
      "Iteration 117, loss = 0.59989315\n",
      "Iteration 118, loss = 0.59913526\n",
      "Iteration 119, loss = 0.59838378\n",
      "Iteration 120, loss = 0.59763848\n",
      "Iteration 121, loss = 0.59689914\n",
      "Iteration 122, loss = 0.59616556\n",
      "Iteration 123, loss = 0.59543756\n",
      "Iteration 124, loss = 0.59471498\n",
      "Iteration 125, loss = 0.59399768\n",
      "Iteration 126, loss = 0.59328551\n",
      "Iteration 127, loss = 0.59257835\n",
      "Iteration 128, loss = 0.59187609\n",
      "Iteration 129, loss = 0.59117861\n",
      "Iteration 130, loss = 0.59048582\n",
      "Iteration 131, loss = 0.58979761\n",
      "Iteration 132, loss = 0.58911389\n",
      "Iteration 133, loss = 0.58847723\n",
      "Iteration 134, loss = 0.58786269\n",
      "Iteration 135, loss = 0.58724702\n",
      "Iteration 136, loss = 0.58663109\n",
      "Iteration 137, loss = 0.58601569\n",
      "Iteration 138, loss = 0.58540152\n",
      "Iteration 139, loss = 0.58478919\n",
      "Iteration 140, loss = 0.58417927\n",
      "Iteration 141, loss = 0.58357222\n",
      "Iteration 142, loss = 0.58296848\n",
      "Iteration 143, loss = 0.58240729\n",
      "Iteration 144, loss = 0.58183690\n",
      "Iteration 145, loss = 0.58126417\n",
      "Iteration 146, loss = 0.58068939\n",
      "Iteration 147, loss = 0.58011820\n",
      "Iteration 148, loss = 0.57956263\n",
      "Iteration 149, loss = 0.57901694\n",
      "Iteration 150, loss = 0.57847310\n",
      "Iteration 151, loss = 0.57793005\n",
      "Iteration 152, loss = 0.57739641\n",
      "Iteration 153, loss = 0.57686558\n",
      "Iteration 154, loss = 0.57633528\n",
      "Iteration 155, loss = 0.57582167\n",
      "Iteration 156, loss = 0.57529599\n",
      "Iteration 157, loss = 0.57478516\n",
      "Iteration 158, loss = 0.57427774\n",
      "Iteration 159, loss = 0.57377018\n",
      "Iteration 160, loss = 0.57326414\n",
      "Iteration 161, loss = 0.57277933\n",
      "Iteration 162, loss = 0.57227279\n",
      "Iteration 163, loss = 0.57179288\n",
      "Iteration 164, loss = 0.57130654\n",
      "Iteration 165, loss = 0.57082137\n",
      "Iteration 166, loss = 0.57033767\n",
      "Iteration 167, loss = 0.56986067\n",
      "Iteration 168, loss = 0.56938932\n",
      "Iteration 169, loss = 0.56891697\n",
      "Iteration 170, loss = 0.56845625\n",
      "Iteration 171, loss = 0.56799036\n",
      "Iteration 172, loss = 0.56752867\n",
      "Iteration 173, loss = 0.56707357\n",
      "Iteration 174, loss = 0.56662245\n",
      "Iteration 175, loss = 0.56617979\n",
      "Iteration 176, loss = 0.56578166\n",
      "Iteration 177, loss = 0.56541009\n",
      "Iteration 178, loss = 0.56502235\n",
      "Iteration 179, loss = 0.56464001\n",
      "Iteration 180, loss = 0.56426318\n",
      "Iteration 181, loss = 0.56390102\n",
      "Iteration 182, loss = 0.56352912\n",
      "Iteration 183, loss = 0.56316078\n",
      "Iteration 184, loss = 0.56280258\n",
      "Iteration 185, loss = 0.56243705\n",
      "Iteration 186, loss = 0.56209394\n",
      "Iteration 187, loss = 0.56173017\n",
      "Iteration 188, loss = 0.56138317\n",
      "Iteration 189, loss = 0.56103104\n",
      "Iteration 190, loss = 0.56067742\n",
      "Iteration 191, loss = 0.56034559\n",
      "Iteration 192, loss = 0.55999186\n",
      "Iteration 193, loss = 0.55965670\n",
      "Iteration 194, loss = 0.55931530\n",
      "Iteration 195, loss = 0.55897229\n",
      "Iteration 196, loss = 0.55864345\n",
      "Iteration 197, loss = 0.55830117\n",
      "Iteration 198, loss = 0.55797392\n",
      "Iteration 199, loss = 0.55764176\n",
      "Iteration 200, loss = 0.55730912\n",
      "Iteration 201, loss = 0.55698593\n",
      "Iteration 202, loss = 0.55665758\n",
      "Iteration 203, loss = 0.55632907\n",
      "Iteration 204, loss = 0.55601400\n",
      "Iteration 205, loss = 0.55568570\n",
      "Iteration 206, loss = 0.55536221\n",
      "Iteration 207, loss = 0.55505282\n",
      "Iteration 208, loss = 0.55473337\n",
      "Iteration 209, loss = 0.55441244\n",
      "Iteration 210, loss = 0.55409583\n",
      "Iteration 211, loss = 0.55378408\n",
      "Iteration 212, loss = 0.55346751\n",
      "Iteration 213, loss = 0.55316044\n",
      "Iteration 214, loss = 0.55284753\n",
      "Iteration 215, loss = 0.55253515\n",
      "Iteration 216, loss = 0.55223203\n",
      "Iteration 217, loss = 0.55192322\n",
      "Iteration 218, loss = 0.55161486\n",
      "Iteration 219, loss = 0.55130970\n",
      "Iteration 220, loss = 0.55101063\n",
      "Iteration 221, loss = 0.55070612\n",
      "Iteration 222, loss = 0.55040044\n",
      "Iteration 223, loss = 0.55010606\n",
      "Iteration 224, loss = 0.54980196\n",
      "Iteration 225, loss = 0.54950067\n",
      "Iteration 226, loss = 0.54920852\n",
      "Iteration 227, loss = 0.54891044\n",
      "Iteration 228, loss = 0.54861316\n",
      "Iteration 229, loss = 0.54831484\n",
      "Iteration 230, loss = 0.54803175\n",
      "Iteration 231, loss = 0.54773072\n",
      "Iteration 232, loss = 0.54743675\n",
      "Iteration 233, loss = 0.54715046\n",
      "Iteration 234, loss = 0.54686074\n",
      "Iteration 235, loss = 0.54657067\n",
      "Iteration 236, loss = 0.54627959\n",
      "Iteration 237, loss = 0.54599212\n",
      "Iteration 238, loss = 0.54570946\n",
      "Iteration 239, loss = 0.54542253\n",
      "Iteration 240, loss = 0.54513468\n",
      "Iteration 241, loss = 0.54485144\n",
      "Iteration 242, loss = 0.54457099\n",
      "Iteration 243, loss = 0.54428739\n",
      "Iteration 244, loss = 0.54400295\n",
      "Iteration 245, loss = 0.54372446\n",
      "Iteration 246, loss = 0.54344593\n",
      "Iteration 247, loss = 0.54316575\n",
      "Iteration 248, loss = 0.54288475\n",
      "Iteration 249, loss = 0.54260843\n",
      "Iteration 250, loss = 0.54233445\n",
      "Iteration 251, loss = 0.54205766\n",
      "Iteration 252, loss = 0.54178007\n",
      "Iteration 253, loss = 0.54150188\n",
      "Iteration 254, loss = 0.54124388\n",
      "Iteration 255, loss = 0.54095743\n",
      "Iteration 256, loss = 0.54068382\n",
      "Iteration 257, loss = 0.54041710\n",
      "Iteration 258, loss = 0.54014770\n",
      "Iteration 259, loss = 0.53987797\n",
      "Iteration 260, loss = 0.53960740\n",
      "Iteration 261, loss = 0.53933617\n",
      "Iteration 262, loss = 0.53906464\n",
      "Iteration 263, loss = 0.53880511\n",
      "Iteration 264, loss = 0.53853815\n",
      "Iteration 265, loss = 0.53827049\n",
      "Iteration 266, loss = 0.53800229\n",
      "Iteration 267, loss = 0.53774152\n",
      "Iteration 268, loss = 0.53747733\n",
      "Iteration 269, loss = 0.53721355\n",
      "Iteration 270, loss = 0.53694916\n",
      "Iteration 271, loss = 0.53668476\n",
      "Iteration 272, loss = 0.53643137\n",
      "Iteration 273, loss = 0.53617108\n",
      "Iteration 274, loss = 0.53591010\n",
      "Iteration 275, loss = 0.53564858\n",
      "Iteration 276, loss = 0.53538668\n",
      "Iteration 277, loss = 0.53514050\n",
      "Iteration 278, loss = 0.53487423\n",
      "Iteration 279, loss = 0.53461690\n",
      "Iteration 280, loss = 0.53435902\n",
      "Iteration 281, loss = 0.53411149\n",
      "Iteration 282, loss = 0.53385410\n",
      "Iteration 283, loss = 0.53360037\n",
      "Iteration 284, loss = 0.53334599\n",
      "Iteration 285, loss = 0.53309112\n",
      "Iteration 286, loss = 0.53283590\n",
      "Iteration 287, loss = 0.53260063\n",
      "Iteration 288, loss = 0.53233652\n",
      "Iteration 289, loss = 0.53208577\n",
      "Iteration 290, loss = 0.53183449\n",
      "Iteration 291, loss = 0.53158980\n",
      "Iteration 292, loss = 0.53134246\n",
      "Iteration 293, loss = 0.53109520\n",
      "Iteration 294, loss = 0.53084731\n",
      "Iteration 295, loss = 0.53059892\n",
      "Iteration 296, loss = 0.53035016\n",
      "Iteration 297, loss = 0.53010370\n",
      "Iteration 298, loss = 0.52986342\n",
      "Iteration 299, loss = 0.52961898\n",
      "Iteration 300, loss = 0.52937401\n",
      "Iteration 301, loss = 0.52912863\n",
      "Iteration 302, loss = 0.52888296\n",
      "Iteration 303, loss = 0.52865273\n",
      "Iteration 304, loss = 0.52840244\n",
      "Iteration 305, loss = 0.52816120\n",
      "Iteration 306, loss = 0.52791948\n",
      "Iteration 307, loss = 0.52767740\n",
      "Iteration 308, loss = 0.52744787\n",
      "Iteration 309, loss = 0.52720374\n",
      "Iteration 310, loss = 0.52696586\n",
      "Iteration 311, loss = 0.52672744\n",
      "Iteration 312, loss = 0.52648862\n",
      "Iteration 313, loss = 0.52624950\n",
      "Iteration 314, loss = 0.52602267\n",
      "Iteration 315, loss = 0.52578181\n",
      "Iteration 316, loss = 0.52554697\n",
      "Iteration 317, loss = 0.52531165\n",
      "Iteration 318, loss = 0.52507595\n",
      "Iteration 319, loss = 0.52483998\n",
      "Iteration 320, loss = 0.52462065\n",
      "Iteration 321, loss = 0.52437855\n",
      "Iteration 322, loss = 0.52414688\n",
      "Iteration 323, loss = 0.52391474\n",
      "Iteration 324, loss = 0.52368224\n",
      "Iteration 325, loss = 0.52345085\n",
      "Iteration 326, loss = 0.52322745\n",
      "Iteration 327, loss = 0.52299900\n",
      "Iteration 328, loss = 0.52282649\n",
      "Iteration 329, loss = 0.52264252\n",
      "Iteration 330, loss = 0.52249321\n",
      "Iteration 331, loss = 0.52232991\n",
      "Iteration 332, loss = 0.52215671\n",
      "Iteration 333, loss = 0.52197460\n",
      "Iteration 334, loss = 0.52178451\n",
      "Iteration 335, loss = 0.52158727\n",
      "Iteration 336, loss = 0.52138563\n",
      "Iteration 337, loss = 0.52119499\n",
      "Iteration 338, loss = 0.52098964\n",
      "Iteration 339, loss = 0.52082085\n",
      "Iteration 340, loss = 0.52065441\n",
      "Iteration 341, loss = 0.52048294\n",
      "Iteration 342, loss = 0.52030741\n",
      "Iteration 343, loss = 0.52012723\n",
      "Iteration 344, loss = 0.51995598\n",
      "Iteration 345, loss = 0.51978854\n",
      "Iteration 346, loss = 0.51959654\n",
      "Iteration 347, loss = 0.51946815\n",
      "Iteration 348, loss = 0.51925846\n",
      "Iteration 349, loss = 0.51909521\n",
      "Iteration 350, loss = 0.51894081\n",
      "Iteration 351, loss = 0.51877113\n",
      "Iteration 352, loss = 0.51860692\n",
      "Iteration 353, loss = 0.51843457\n",
      "Iteration 354, loss = 0.51825870\n",
      "Iteration 355, loss = 0.51810242\n",
      "Iteration 356, loss = 0.51793997\n",
      "Iteration 357, loss = 0.51779061\n",
      "Iteration 358, loss = 0.51762213\n",
      "Iteration 359, loss = 0.51745173\n",
      "Iteration 360, loss = 0.51728278\n",
      "Iteration 361, loss = 0.51711962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 362, loss = 0.51697007\n",
      "Iteration 363, loss = 0.51681826\n",
      "Iteration 364, loss = 0.51665457\n",
      "Iteration 365, loss = 0.51651435\n",
      "Iteration 366, loss = 0.51633968\n",
      "Iteration 367, loss = 0.51618341\n",
      "Iteration 368, loss = 0.51604401\n",
      "Iteration 369, loss = 0.51587524\n",
      "Iteration 370, loss = 0.51571652\n",
      "Iteration 371, loss = 0.51558609\n",
      "Iteration 372, loss = 0.51541491\n",
      "Iteration 373, loss = 0.51526488\n",
      "Iteration 374, loss = 0.51511339\n",
      "Iteration 375, loss = 0.51494823\n",
      "Iteration 376, loss = 0.51478944\n",
      "Iteration 377, loss = 0.51468933\n",
      "Iteration 378, loss = 0.51449173\n",
      "Iteration 379, loss = 0.51435955\n",
      "Iteration 380, loss = 0.51420075\n",
      "Iteration 381, loss = 0.51404517\n",
      "Iteration 382, loss = 0.51390262\n",
      "Iteration 383, loss = 0.51377609\n",
      "Iteration 384, loss = 0.51361948\n",
      "Iteration 385, loss = 0.51346923\n",
      "Iteration 386, loss = 0.51331653\n",
      "Iteration 387, loss = 0.51318638\n",
      "Iteration 388, loss = 0.51303853\n",
      "Iteration 389, loss = 0.51287747\n",
      "Iteration 390, loss = 0.51274020\n",
      "Iteration 391, loss = 0.51260364\n",
      "Iteration 392, loss = 0.51246731\n",
      "Iteration 393, loss = 0.51230798\n",
      "Iteration 394, loss = 0.51218697\n",
      "Iteration 395, loss = 0.51203162\n",
      "Iteration 396, loss = 0.51187929\n",
      "Iteration 397, loss = 0.51173771\n",
      "Iteration 398, loss = 0.51162925\n",
      "Iteration 399, loss = 0.51147079\n",
      "Iteration 400, loss = 0.51132113\n",
      "Iteration 401, loss = 0.51118762\n",
      "Iteration 402, loss = 0.51104804\n",
      "Iteration 403, loss = 0.51093048\n",
      "Iteration 404, loss = 0.51078000\n",
      "Iteration 405, loss = 0.51064958\n",
      "Iteration 406, loss = 0.51050513\n",
      "Iteration 407, loss = 0.51035394\n",
      "Iteration 408, loss = 0.51022514\n",
      "Iteration 409, loss = 0.51010928\n",
      "Iteration 410, loss = 0.50996446\n",
      "Iteration 411, loss = 0.50981573\n",
      "Iteration 412, loss = 0.50969929\n",
      "Iteration 413, loss = 0.50957006\n",
      "Iteration 414, loss = 0.50942627\n",
      "Iteration 415, loss = 0.50929253\n",
      "Iteration 416, loss = 0.50915060\n",
      "Iteration 417, loss = 0.50901855\n",
      "Iteration 418, loss = 0.50890375\n",
      "Iteration 419, loss = 0.50877528\n",
      "Iteration 420, loss = 0.50863352\n",
      "Iteration 421, loss = 0.50850937\n",
      "Iteration 422, loss = 0.50836113\n",
      "Iteration 423, loss = 0.50823755\n",
      "Iteration 424, loss = 0.50811754\n",
      "Iteration 425, loss = 0.50798219\n",
      "Iteration 426, loss = 0.50784761\n",
      "Iteration 427, loss = 0.50772109\n",
      "Iteration 428, loss = 0.50761617\n",
      "Iteration 429, loss = 0.50748083\n",
      "Iteration 430, loss = 0.50733440\n",
      "Iteration 431, loss = 0.50723698\n",
      "Iteration 432, loss = 0.50708626\n",
      "Iteration 433, loss = 0.50695156\n",
      "Iteration 434, loss = 0.50682816\n",
      "Iteration 435, loss = 0.50671698\n",
      "Iteration 436, loss = 0.50659982\n",
      "Iteration 437, loss = 0.50646699\n",
      "Iteration 438, loss = 0.50633130\n",
      "Iteration 439, loss = 0.50620780\n",
      "Iteration 440, loss = 0.50608490\n",
      "Iteration 441, loss = 0.50597966\n",
      "Iteration 442, loss = 0.50584763\n",
      "Iteration 443, loss = 0.50571855\n",
      "Iteration 444, loss = 0.50558680\n",
      "Iteration 445, loss = 0.50548315\n",
      "Iteration 446, loss = 0.50535844\n",
      "Iteration 447, loss = 0.50522702\n",
      "Iteration 448, loss = 0.50510874\n",
      "Iteration 449, loss = 0.50497233\n",
      "Iteration 450, loss = 0.50486135\n",
      "Iteration 451, loss = 0.50476055\n",
      "Iteration 452, loss = 0.50462320\n",
      "Iteration 453, loss = 0.50451523\n",
      "Iteration 454, loss = 0.50438432\n",
      "Iteration 455, loss = 0.50426181\n",
      "Iteration 456, loss = 0.50416541\n",
      "Iteration 457, loss = 0.50403924\n",
      "Iteration 458, loss = 0.50391258\n",
      "Iteration 459, loss = 0.50379309\n",
      "Iteration 460, loss = 0.50367557\n",
      "Iteration 461, loss = 0.50354390\n",
      "Iteration 462, loss = 0.50347339\n",
      "Iteration 463, loss = 0.50332813\n",
      "Iteration 464, loss = 0.50320418\n",
      "Iteration 465, loss = 0.50309926\n",
      "Iteration 466, loss = 0.50296515\n",
      "Iteration 467, loss = 0.50287517\n",
      "Iteration 468, loss = 0.50275837\n",
      "Iteration 469, loss = 0.50263876\n",
      "Iteration 470, loss = 0.50250997\n",
      "Iteration 471, loss = 0.50241226\n",
      "Iteration 472, loss = 0.50229161\n",
      "Iteration 473, loss = 0.50219219\n",
      "Iteration 474, loss = 0.50207680\n",
      "Iteration 475, loss = 0.50195213\n",
      "Iteration 476, loss = 0.50182285\n",
      "Iteration 477, loss = 0.50171143\n",
      "Iteration 478, loss = 0.50162976\n",
      "Iteration 479, loss = 0.50150511\n",
      "Iteration 480, loss = 0.50138691\n",
      "Iteration 481, loss = 0.50127807\n",
      "Iteration 482, loss = 0.50116014\n",
      "Iteration 483, loss = 0.50103765\n",
      "Iteration 484, loss = 0.50093542\n",
      "Iteration 485, loss = 0.50084054\n",
      "Iteration 486, loss = 0.50071895\n",
      "Iteration 487, loss = 0.50061468\n",
      "Iteration 488, loss = 0.50049488\n",
      "Iteration 489, loss = 0.50038497\n",
      "Iteration 490, loss = 0.50029812\n",
      "Iteration 491, loss = 0.50020597\n",
      "Iteration 492, loss = 0.50010614\n",
      "Iteration 493, loss = 0.50006927\n",
      "Iteration 494, loss = 0.49995281\n",
      "Iteration 495, loss = 0.49987233\n",
      "Iteration 496, loss = 0.49980550\n",
      "Iteration 497, loss = 0.49972118\n",
      "Iteration 498, loss = 0.49961130\n",
      "Iteration 499, loss = 0.49954842\n",
      "Iteration 500, loss = 0.49943334\n",
      "Iteration 501, loss = 0.49934455\n",
      "Iteration 502, loss = 0.49926816\n",
      "Iteration 503, loss = 0.49919619\n",
      "Iteration 504, loss = 0.49911152\n",
      "Iteration 505, loss = 0.49901855\n",
      "Iteration 506, loss = 0.49896483\n",
      "Iteration 507, loss = 0.49890434\n",
      "Iteration 508, loss = 0.49881121\n",
      "Iteration 509, loss = 0.49872861\n",
      "Iteration 510, loss = 0.49863781\n",
      "Iteration 511, loss = 0.49855476\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(2, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1400, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_1[data.columns.drop('WillWait')]\n",
    "y = data_1['WillWait']\n",
    "clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 2), (2, 2), (2, 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[coef.shape for coef in clf.coefs_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.11734811,  0.32287869],\n",
       "        [-0.70694207, -0.43083082],\n",
       "        [-0.49956052, -0.63568953],\n",
       "        [-0.4436932 ,  0.01108255],\n",
       "        [-0.14599223,  0.22750149],\n",
       "        [-0.11427573,  0.50272469],\n",
       "        [-0.41796589,  0.52183514],\n",
       "        [-0.66837207,  0.4705682 ],\n",
       "        [-0.11694818,  0.02986878],\n",
       "        [-0.50856754, -0.48611883]]), array([[-0.45701365,  0.4710903 ],\n",
       "        [ 0.48406208,  1.3890567 ]]), array([[-0.84837649],\n",
       "        [ 1.45168459]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srazvan/anaconda3/envs/radu1/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/srazvan/anaconda3/envs/radu1/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -1.        , -0.84515425,  0.84515425,  1.29986737,\n",
       "         1.56892908, -0.84515425,  1.18321596, -0.39735971, -0.8660254 ],\n",
       "       [ 1.        , -1.        , -0.84515425,  0.84515425, -0.92847669,\n",
       "        -0.78446454, -0.84515425, -0.84515425,  1.19207912,  0.8660254 ],\n",
       "       [-1.        ,  1.        , -0.84515425, -1.18321596,  1.29986737,\n",
       "        -0.78446454, -0.84515425, -0.84515425, -1.19207912, -0.8660254 ],\n",
       "       [ 1.        , -1.        ,  1.18321596,  0.84515425, -0.92847669,\n",
       "        -0.78446454,  1.18321596, -0.84515425,  1.19207912,  0.        ],\n",
       "       [ 1.        , -1.        ,  1.18321596, -1.18321596, -0.92847669,\n",
       "         1.56892908, -0.84515425,  1.18321596, -0.39735971,  1.73205081],\n",
       "       [-1.        ,  1.        , -0.84515425,  0.84515425,  1.29986737,\n",
       "         0.39223227,  1.18321596,  1.18321596,  0.39735971, -0.8660254 ],\n",
       "       [-1.        ,  1.        , -0.84515425, -1.18321596,  0.18569534,\n",
       "        -0.78446454,  1.18321596, -0.84515425, -1.19207912, -0.8660254 ],\n",
       "       [-1.        , -1.        , -0.84515425,  0.84515425,  1.29986737,\n",
       "         0.39223227,  1.18321596,  1.18321596,  1.19207912, -0.8660254 ],\n",
       "       [-1.        ,  1.        ,  1.18321596, -1.18321596, -0.92847669,\n",
       "        -0.78446454,  1.18321596, -0.84515425, -1.19207912,  1.73205081],\n",
       "       [ 1.        ,  1.        ,  1.18321596,  0.84515425, -0.92847669,\n",
       "         1.56892908, -0.84515425,  1.18321596,  0.39735971,  0.        ],\n",
       "       [-1.        , -1.        , -0.84515425, -1.18321596,  0.18569534,\n",
       "        -0.78446454, -0.84515425, -0.84515425,  1.19207912, -0.8660254 ],\n",
       "       [ 1.        ,  1.        ,  1.18321596,  0.84515425, -0.92847669,\n",
       "        -0.78446454, -0.84515425, -0.84515425, -1.19207912,  0.8660254 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended \n",
    "#to scale your data. For example, scale each attribute on the input vector X to [0, 1] or [-1, +1],\n",
    "#or standardize it to have mean 0 and variance 1. \n",
    "#Note that you must apply the same scaling to the test set for meaningful results. \n",
    "#You can use StandardScaler for standardization\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(x)  \n",
    "x = scaler.transform(x)  \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.85003678\n",
      "Iteration 2, loss = 0.84917997\n",
      "Iteration 3, loss = 0.84796172\n",
      "Iteration 4, loss = 0.84642207\n",
      "Iteration 5, loss = 0.84459778\n",
      "Iteration 6, loss = 0.84252255\n",
      "Iteration 7, loss = 0.84022713\n",
      "Iteration 8, loss = 0.83773955\n",
      "Iteration 9, loss = 0.83508532\n",
      "Iteration 10, loss = 0.83228757\n",
      "Iteration 11, loss = 0.82936725\n",
      "Iteration 12, loss = 0.82634330\n",
      "Iteration 13, loss = 0.82323280\n",
      "Iteration 14, loss = 0.82005114\n",
      "Iteration 15, loss = 0.81681215\n",
      "Iteration 16, loss = 0.81352823\n",
      "Iteration 17, loss = 0.81021047\n",
      "Iteration 18, loss = 0.80686880\n",
      "Iteration 19, loss = 0.80351204\n",
      "Iteration 20, loss = 0.80014806\n",
      "Iteration 21, loss = 0.79678381\n",
      "Iteration 22, loss = 0.79342545\n",
      "Iteration 23, loss = 0.79007841\n",
      "Iteration 24, loss = 0.78674745\n",
      "Iteration 25, loss = 0.78343673\n",
      "Iteration 26, loss = 0.78014990\n",
      "Iteration 27, loss = 0.77689008\n",
      "Iteration 28, loss = 0.77365998\n",
      "Iteration 29, loss = 0.77046192\n",
      "Iteration 30, loss = 0.76729785\n",
      "Iteration 31, loss = 0.76416942\n",
      "Iteration 32, loss = 0.76107799\n",
      "Iteration 33, loss = 0.75802466\n",
      "Iteration 34, loss = 0.75501031\n",
      "Iteration 35, loss = 0.75203561\n",
      "Iteration 36, loss = 0.74910107\n",
      "Iteration 37, loss = 0.74620702\n",
      "Iteration 38, loss = 0.74335366\n",
      "Iteration 39, loss = 0.74054106\n",
      "Iteration 40, loss = 0.73776918\n",
      "Iteration 41, loss = 0.73503789\n",
      "Iteration 42, loss = 0.73234697\n",
      "Iteration 43, loss = 0.72969612\n",
      "Iteration 44, loss = 0.72708499\n",
      "Iteration 45, loss = 0.72451316\n",
      "Iteration 46, loss = 0.72198016\n",
      "Iteration 47, loss = 0.71948551\n",
      "Iteration 48, loss = 0.71702867\n",
      "Iteration 49, loss = 0.71460906\n",
      "Iteration 50, loss = 0.71222610\n",
      "Iteration 51, loss = 0.70987919\n",
      "Iteration 52, loss = 0.70756770\n",
      "Iteration 53, loss = 0.70529101\n",
      "Iteration 54, loss = 0.70304846\n",
      "Iteration 55, loss = 0.70083942\n",
      "Iteration 56, loss = 0.69866324\n",
      "Iteration 57, loss = 0.69651925\n",
      "Iteration 58, loss = 0.69440683\n",
      "Iteration 59, loss = 0.69232532\n",
      "Iteration 60, loss = 0.69027408\n",
      "Iteration 61, loss = 0.68825248\n",
      "Iteration 62, loss = 0.68625989\n",
      "Iteration 63, loss = 0.68429569\n",
      "Iteration 64, loss = 0.68235927\n",
      "Iteration 65, loss = 0.68045003\n",
      "Iteration 66, loss = 0.67856738\n",
      "Iteration 67, loss = 0.67671074\n",
      "Iteration 68, loss = 0.67487953\n",
      "Iteration 69, loss = 0.67307320\n",
      "Iteration 70, loss = 0.67129120\n",
      "Iteration 71, loss = 0.66953301\n",
      "Iteration 72, loss = 0.66779808\n",
      "Iteration 73, loss = 0.66608592\n",
      "Iteration 74, loss = 0.66439603\n",
      "Iteration 75, loss = 0.66272791\n",
      "Iteration 76, loss = 0.66108109\n",
      "Iteration 77, loss = 0.65945511\n",
      "Iteration 78, loss = 0.65784951\n",
      "Iteration 79, loss = 0.65626386\n",
      "Iteration 80, loss = 0.65469772\n",
      "Iteration 81, loss = 0.65315068\n",
      "Iteration 82, loss = 0.65162233\n",
      "Iteration 83, loss = 0.65011226\n",
      "Iteration 84, loss = 0.64862011\n",
      "Iteration 85, loss = 0.64714548\n",
      "Iteration 86, loss = 0.64568802\n",
      "Iteration 87, loss = 0.64424737\n",
      "Iteration 88, loss = 0.64282318\n",
      "Iteration 89, loss = 0.64141511\n",
      "Iteration 90, loss = 0.64002285\n",
      "Iteration 91, loss = 0.63864606\n",
      "Iteration 92, loss = 0.63728445\n",
      "Iteration 93, loss = 0.63593770\n",
      "Iteration 94, loss = 0.63460553\n",
      "Iteration 95, loss = 0.63328765\n",
      "Iteration 96, loss = 0.63198378\n",
      "Iteration 97, loss = 0.63069366\n",
      "Iteration 98, loss = 0.62941702\n",
      "Iteration 99, loss = 0.62815360\n",
      "Iteration 100, loss = 0.62690316\n",
      "Iteration 101, loss = 0.62566545\n",
      "Iteration 102, loss = 0.62444024\n",
      "Iteration 103, loss = 0.62322730\n",
      "Iteration 104, loss = 0.62202641\n",
      "Iteration 105, loss = 0.62083734\n",
      "Iteration 106, loss = 0.61965989\n",
      "Iteration 107, loss = 0.61849386\n",
      "Iteration 108, loss = 0.61733903\n",
      "Iteration 109, loss = 0.61619522\n",
      "Iteration 110, loss = 0.61506224\n",
      "Iteration 111, loss = 0.61393989\n",
      "Iteration 112, loss = 0.61282799\n",
      "Iteration 113, loss = 0.61172638\n",
      "Iteration 114, loss = 0.61063488\n",
      "Iteration 115, loss = 0.60955331\n",
      "Iteration 116, loss = 0.60848152\n",
      "Iteration 117, loss = 0.60741935\n",
      "Iteration 118, loss = 0.60636663\n",
      "Iteration 119, loss = 0.60532323\n",
      "Iteration 120, loss = 0.60428898\n",
      "Iteration 121, loss = 0.60326374\n",
      "Iteration 122, loss = 0.60224737\n",
      "Iteration 123, loss = 0.60123973\n",
      "Iteration 124, loss = 0.60024069\n",
      "Iteration 125, loss = 0.59925012\n",
      "Iteration 126, loss = 0.59826787\n",
      "Iteration 127, loss = 0.59729383\n",
      "Iteration 128, loss = 0.59632788\n",
      "Iteration 129, loss = 0.59536988\n",
      "Iteration 130, loss = 0.59441973\n",
      "Iteration 131, loss = 0.59347730\n",
      "Iteration 132, loss = 0.59254248\n",
      "Iteration 133, loss = 0.59161517\n",
      "Iteration 134, loss = 0.59069524\n",
      "Iteration 135, loss = 0.58978260\n",
      "Iteration 136, loss = 0.58887714\n",
      "Iteration 137, loss = 0.58797876\n",
      "Iteration 138, loss = 0.58708735\n",
      "Iteration 139, loss = 0.58620282\n",
      "Iteration 140, loss = 0.58532507\n",
      "Iteration 141, loss = 0.58445400\n",
      "Iteration 142, loss = 0.58358953\n",
      "Iteration 143, loss = 0.58273156\n",
      "Iteration 144, loss = 0.58188944\n",
      "Iteration 145, loss = 0.58105696\n",
      "Iteration 146, loss = 0.58023058\n",
      "Iteration 147, loss = 0.57941016\n",
      "Iteration 148, loss = 0.57859559\n",
      "Iteration 149, loss = 0.57778674\n",
      "Iteration 150, loss = 0.57698350\n",
      "Iteration 151, loss = 0.57618578\n",
      "Iteration 152, loss = 0.57539348\n",
      "Iteration 153, loss = 0.57460651\n",
      "Iteration 154, loss = 0.57382477\n",
      "Iteration 155, loss = 0.57304818\n",
      "Iteration 156, loss = 0.57227668\n",
      "Iteration 157, loss = 0.57151017\n",
      "Iteration 158, loss = 0.57074859\n",
      "Iteration 159, loss = 0.56999186\n",
      "Iteration 160, loss = 0.56923992\n",
      "Iteration 161, loss = 0.56849270\n",
      "Iteration 162, loss = 0.56775014\n",
      "Iteration 163, loss = 0.56701218\n",
      "Iteration 164, loss = 0.56627876\n",
      "Iteration 165, loss = 0.56554982\n",
      "Iteration 166, loss = 0.56482530\n",
      "Iteration 167, loss = 0.56410515\n",
      "Iteration 168, loss = 0.56338932\n",
      "Iteration 169, loss = 0.56267776\n",
      "Iteration 170, loss = 0.56197040\n",
      "Iteration 171, loss = 0.56126722\n",
      "Iteration 172, loss = 0.56056814\n",
      "Iteration 173, loss = 0.55987314\n",
      "Iteration 174, loss = 0.55918215\n",
      "Iteration 175, loss = 0.55849515\n",
      "Iteration 176, loss = 0.55781207\n",
      "Iteration 177, loss = 0.55713287\n",
      "Iteration 178, loss = 0.55645752\n",
      "Iteration 179, loss = 0.55578597\n",
      "Iteration 180, loss = 0.55511818\n",
      "Iteration 181, loss = 0.55445411\n",
      "Iteration 182, loss = 0.55379370\n",
      "Iteration 183, loss = 0.55313694\n",
      "Iteration 184, loss = 0.55248377\n",
      "Iteration 185, loss = 0.55183416\n",
      "Iteration 186, loss = 0.55118806\n",
      "Iteration 187, loss = 0.55054545\n",
      "Iteration 188, loss = 0.54990628\n",
      "Iteration 189, loss = 0.54927052\n",
      "Iteration 190, loss = 0.54863812\n",
      "Iteration 191, loss = 0.54800907\n",
      "Iteration 192, loss = 0.54738331\n",
      "Iteration 193, loss = 0.54676081\n",
      "Iteration 194, loss = 0.54614154\n",
      "Iteration 195, loss = 0.54552547\n",
      "Iteration 196, loss = 0.54491256\n",
      "Iteration 197, loss = 0.54430278\n",
      "Iteration 198, loss = 0.54369610\n",
      "Iteration 199, loss = 0.54309248\n",
      "Iteration 200, loss = 0.54249189\n",
      "Iteration 201, loss = 0.54189430\n",
      "Iteration 202, loss = 0.54129968\n",
      "Iteration 203, loss = 0.54070800\n",
      "Iteration 204, loss = 0.54011922\n",
      "Iteration 205, loss = 0.53953332\n",
      "Iteration 206, loss = 0.53895027\n",
      "Iteration 207, loss = 0.53837004\n",
      "Iteration 208, loss = 0.53779260\n",
      "Iteration 209, loss = 0.53721793\n",
      "Iteration 210, loss = 0.53664598\n",
      "Iteration 211, loss = 0.53607674\n",
      "Iteration 212, loss = 0.53551017\n",
      "Iteration 213, loss = 0.53494626\n",
      "Iteration 214, loss = 0.53438497\n",
      "Iteration 215, loss = 0.53382628\n",
      "Iteration 216, loss = 0.53327015\n",
      "Iteration 217, loss = 0.53271658\n",
      "Iteration 218, loss = 0.53216552\n",
      "Iteration 219, loss = 0.53161695\n",
      "Iteration 220, loss = 0.53107085\n",
      "Iteration 221, loss = 0.53052720\n",
      "Iteration 222, loss = 0.52998597\n",
      "Iteration 223, loss = 0.52944713\n",
      "Iteration 224, loss = 0.52891066\n",
      "Iteration 225, loss = 0.52837654\n",
      "Iteration 226, loss = 0.52784475\n",
      "Iteration 227, loss = 0.52731526\n",
      "Iteration 228, loss = 0.52678804\n",
      "Iteration 229, loss = 0.52626309\n",
      "Iteration 230, loss = 0.52574037\n",
      "Iteration 231, loss = 0.52521986\n",
      "Iteration 232, loss = 0.52470154\n",
      "Iteration 233, loss = 0.52418539\n",
      "Iteration 234, loss = 0.52367139\n",
      "Iteration 235, loss = 0.52315952\n",
      "Iteration 236, loss = 0.52264975\n",
      "Iteration 237, loss = 0.52214208\n",
      "Iteration 238, loss = 0.52163646\n",
      "Iteration 239, loss = 0.52113290\n",
      "Iteration 240, loss = 0.52063136\n",
      "Iteration 241, loss = 0.52013184\n",
      "Iteration 242, loss = 0.51963430\n",
      "Iteration 243, loss = 0.51913873\n",
      "Iteration 244, loss = 0.51864511\n",
      "Iteration 245, loss = 0.51815342\n",
      "Iteration 246, loss = 0.51766365\n",
      "Iteration 247, loss = 0.51717578\n",
      "Iteration 248, loss = 0.51668978\n",
      "Iteration 249, loss = 0.51620565\n",
      "Iteration 250, loss = 0.51572336\n",
      "Iteration 251, loss = 0.51524289\n",
      "Iteration 252, loss = 0.51476424\n",
      "Iteration 253, loss = 0.51428738\n",
      "Iteration 254, loss = 0.51381230\n",
      "Iteration 255, loss = 0.51333897\n",
      "Iteration 256, loss = 0.51286740\n",
      "Iteration 257, loss = 0.51239755\n",
      "Iteration 258, loss = 0.51192941\n",
      "Iteration 259, loss = 0.51146297\n",
      "Iteration 260, loss = 0.51099821\n",
      "Iteration 261, loss = 0.51053512\n",
      "Iteration 262, loss = 0.51007368\n",
      "Iteration 263, loss = 0.50961388\n",
      "Iteration 264, loss = 0.50915570\n",
      "Iteration 265, loss = 0.50869913\n",
      "Iteration 266, loss = 0.50824415\n",
      "Iteration 267, loss = 0.50779075\n",
      "Iteration 268, loss = 0.50733892\n",
      "Iteration 269, loss = 0.50688864\n",
      "Iteration 270, loss = 0.50643990\n",
      "Iteration 271, loss = 0.50599268\n",
      "Iteration 272, loss = 0.50554698\n",
      "Iteration 273, loss = 0.50510277\n",
      "Iteration 274, loss = 0.50466005\n",
      "Iteration 275, loss = 0.50421881\n",
      "Iteration 276, loss = 0.50377902\n",
      "Iteration 277, loss = 0.50334068\n",
      "Iteration 278, loss = 0.50290378\n",
      "Iteration 279, loss = 0.50246831\n",
      "Iteration 280, loss = 0.50203424\n",
      "Iteration 281, loss = 0.50160157\n",
      "Iteration 282, loss = 0.50117030\n",
      "Iteration 283, loss = 0.50074039\n",
      "Iteration 284, loss = 0.50031186\n",
      "Iteration 285, loss = 0.49988468\n",
      "Iteration 286, loss = 0.49945884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 287, loss = 0.49903433\n",
      "Iteration 288, loss = 0.49861115\n",
      "Iteration 289, loss = 0.49818927\n",
      "Iteration 290, loss = 0.49776870\n",
      "Iteration 291, loss = 0.49734941\n",
      "Iteration 292, loss = 0.49693140\n",
      "Iteration 293, loss = 0.49651466\n",
      "Iteration 294, loss = 0.49609918\n",
      "Iteration 295, loss = 0.49568495\n",
      "Iteration 296, loss = 0.49527196\n",
      "Iteration 297, loss = 0.49486019\n",
      "Iteration 298, loss = 0.49444965\n",
      "Iteration 299, loss = 0.49404031\n",
      "Iteration 300, loss = 0.49363218\n",
      "Iteration 301, loss = 0.49322524\n",
      "Iteration 302, loss = 0.49281948\n",
      "Iteration 303, loss = 0.49241489\n",
      "Iteration 304, loss = 0.49201147\n",
      "Iteration 305, loss = 0.49160920\n",
      "Iteration 306, loss = 0.49120808\n",
      "Iteration 307, loss = 0.49080809\n",
      "Iteration 308, loss = 0.49040924\n",
      "Iteration 309, loss = 0.49001151\n",
      "Iteration 310, loss = 0.48961489\n",
      "Iteration 311, loss = 0.48921937\n",
      "Iteration 312, loss = 0.48882495\n",
      "Iteration 313, loss = 0.48843162\n",
      "Iteration 314, loss = 0.48803937\n",
      "Iteration 315, loss = 0.48764820\n",
      "Iteration 316, loss = 0.48725808\n",
      "Iteration 317, loss = 0.48686903\n",
      "Iteration 318, loss = 0.48648102\n",
      "Iteration 319, loss = 0.48609406\n",
      "Iteration 320, loss = 0.48570814\n",
      "Iteration 321, loss = 0.48532324\n",
      "Iteration 322, loss = 0.48493936\n",
      "Iteration 323, loss = 0.48455649\n",
      "Iteration 324, loss = 0.48417463\n",
      "Iteration 325, loss = 0.48385677\n",
      "Iteration 326, loss = 0.48353595\n",
      "Iteration 327, loss = 0.48320808\n",
      "Iteration 328, loss = 0.48287386\n",
      "Iteration 329, loss = 0.48253396\n",
      "Iteration 330, loss = 0.48218898\n",
      "Iteration 331, loss = 0.48183951\n",
      "Iteration 332, loss = 0.48148607\n",
      "Iteration 333, loss = 0.48113429\n",
      "Iteration 334, loss = 0.48079485\n",
      "Iteration 335, loss = 0.48045203\n",
      "Iteration 336, loss = 0.48014681\n",
      "Iteration 337, loss = 0.47983998\n",
      "Iteration 338, loss = 0.47953695\n",
      "Iteration 339, loss = 0.47922386\n",
      "Iteration 340, loss = 0.47891900\n",
      "Iteration 341, loss = 0.47860346\n",
      "Iteration 342, loss = 0.47829532\n",
      "Iteration 343, loss = 0.47798430\n",
      "Iteration 344, loss = 0.47766752\n",
      "Iteration 345, loss = 0.47736628\n",
      "Iteration 346, loss = 0.47705134\n",
      "Iteration 347, loss = 0.47675914\n",
      "Iteration 348, loss = 0.47646722\n",
      "Iteration 349, loss = 0.47616168\n",
      "Iteration 350, loss = 0.47586015\n",
      "Iteration 351, loss = 0.47555192\n",
      "Iteration 352, loss = 0.47527859\n",
      "Iteration 353, loss = 0.47496525\n",
      "Iteration 354, loss = 0.47467538\n",
      "Iteration 355, loss = 0.47437708\n",
      "Iteration 356, loss = 0.47408554\n",
      "Iteration 357, loss = 0.47380199\n",
      "Iteration 358, loss = 0.47350400\n",
      "Iteration 359, loss = 0.47322284\n",
      "Iteration 360, loss = 0.47293751\n",
      "Iteration 361, loss = 0.47265488\n",
      "Iteration 362, loss = 0.47238182\n",
      "Iteration 363, loss = 0.47209364\n",
      "Iteration 364, loss = 0.47181568\n",
      "Iteration 365, loss = 0.47153944\n",
      "Iteration 366, loss = 0.47125189\n",
      "Iteration 367, loss = 0.47099542\n",
      "Iteration 368, loss = 0.47070119\n",
      "Iteration 369, loss = 0.47043665\n",
      "Iteration 370, loss = 0.47015784\n",
      "Iteration 371, loss = 0.46988146\n",
      "Iteration 372, loss = 0.46961552\n",
      "Iteration 373, loss = 0.46934070\n",
      "Iteration 374, loss = 0.46907596\n",
      "Iteration 375, loss = 0.46879539\n",
      "Iteration 376, loss = 0.46853685\n",
      "Iteration 377, loss = 0.46826357\n",
      "Iteration 378, loss = 0.46799136\n",
      "Iteration 379, loss = 0.46772957\n",
      "Iteration 380, loss = 0.46746280\n",
      "Iteration 381, loss = 0.46719820\n",
      "Iteration 382, loss = 0.46693921\n",
      "Iteration 383, loss = 0.46667082\n",
      "Iteration 384, loss = 0.46640907\n",
      "Iteration 385, loss = 0.46614547\n",
      "Iteration 386, loss = 0.46589116\n",
      "Iteration 387, loss = 0.46561900\n",
      "Iteration 388, loss = 0.46538002\n",
      "Iteration 389, loss = 0.46509852\n",
      "Iteration 390, loss = 0.46485010\n",
      "Iteration 391, loss = 0.46459570\n",
      "Iteration 392, loss = 0.46432839\n",
      "Iteration 393, loss = 0.46408781\n",
      "Iteration 394, loss = 0.46382248\n",
      "Iteration 395, loss = 0.46357626\n",
      "Iteration 396, loss = 0.46331461\n",
      "Iteration 397, loss = 0.46305156\n",
      "Iteration 398, loss = 0.46280302\n",
      "Iteration 399, loss = 0.46255016\n",
      "Iteration 400, loss = 0.46230499\n",
      "Iteration 401, loss = 0.46205369\n",
      "Iteration 402, loss = 0.46179645\n",
      "Iteration 403, loss = 0.46154981\n",
      "Iteration 404, loss = 0.46129347\n",
      "Iteration 405, loss = 0.46105992\n",
      "Iteration 406, loss = 0.46080168\n",
      "Iteration 407, loss = 0.46056393\n",
      "Iteration 408, loss = 0.46030465\n",
      "Iteration 409, loss = 0.46007433\n",
      "Iteration 410, loss = 0.45980833\n",
      "Iteration 411, loss = 0.45957400\n",
      "Iteration 412, loss = 0.45933265\n",
      "Iteration 413, loss = 0.45908392\n",
      "Iteration 414, loss = 0.45884359\n",
      "Iteration 415, loss = 0.45859935\n",
      "Iteration 416, loss = 0.45835130\n",
      "Iteration 417, loss = 0.45811528\n",
      "Iteration 418, loss = 0.45787561\n",
      "Iteration 419, loss = 0.45763345\n",
      "Iteration 420, loss = 0.45738301\n",
      "Iteration 421, loss = 0.45715272\n",
      "Iteration 422, loss = 0.45690662\n",
      "Iteration 423, loss = 0.45667458\n",
      "Iteration 424, loss = 0.45643372\n",
      "Iteration 425, loss = 0.45620423\n",
      "Iteration 426, loss = 0.45595670\n",
      "Iteration 427, loss = 0.45572421\n",
      "Iteration 428, loss = 0.45548299\n",
      "Iteration 429, loss = 0.45524926\n",
      "Iteration 430, loss = 0.45502310\n",
      "Iteration 431, loss = 0.45478116\n",
      "Iteration 432, loss = 0.45454837\n",
      "Iteration 433, loss = 0.45431919\n",
      "Iteration 434, loss = 0.45407748\n",
      "Iteration 435, loss = 0.45384313\n",
      "Iteration 436, loss = 0.45362932\n",
      "Iteration 437, loss = 0.45337613\n",
      "Iteration 438, loss = 0.45315643\n",
      "Iteration 439, loss = 0.45292772\n",
      "Iteration 440, loss = 0.45269336\n",
      "Iteration 441, loss = 0.45245740\n",
      "Iteration 442, loss = 0.45223491\n",
      "Iteration 443, loss = 0.45199873\n",
      "Iteration 444, loss = 0.45177472\n",
      "Iteration 445, loss = 0.45155777\n",
      "Iteration 446, loss = 0.45131830\n",
      "Iteration 447, loss = 0.45109921\n",
      "Iteration 448, loss = 0.45086708\n",
      "Iteration 449, loss = 0.45064663\n",
      "Iteration 450, loss = 0.45041442\n",
      "Iteration 451, loss = 0.45018792\n",
      "Iteration 452, loss = 0.44996365\n",
      "Iteration 453, loss = 0.44974120\n",
      "Iteration 454, loss = 0.44952460\n",
      "Iteration 455, loss = 0.44929342\n",
      "Iteration 456, loss = 0.44907333\n",
      "Iteration 457, loss = 0.44884994\n",
      "Iteration 458, loss = 0.44863766\n",
      "Iteration 459, loss = 0.44841017\n",
      "Iteration 460, loss = 0.44817800\n",
      "Iteration 461, loss = 0.44797462\n",
      "Iteration 462, loss = 0.44774740\n",
      "Iteration 463, loss = 0.44752909\n",
      "Iteration 464, loss = 0.44730401\n",
      "Iteration 465, loss = 0.44708814\n",
      "Iteration 466, loss = 0.44687179\n",
      "Iteration 467, loss = 0.44665234\n",
      "Iteration 468, loss = 0.44643191\n",
      "Iteration 469, loss = 0.44620650\n",
      "Iteration 470, loss = 0.44600662\n",
      "Iteration 471, loss = 0.44577855\n",
      "Iteration 472, loss = 0.44557606\n",
      "Iteration 473, loss = 0.44535048\n",
      "Iteration 474, loss = 0.44513406\n",
      "Iteration 475, loss = 0.44492041\n",
      "Iteration 476, loss = 0.44470964\n",
      "Iteration 477, loss = 0.44449795\n",
      "Iteration 478, loss = 0.44427125\n",
      "Iteration 479, loss = 0.44406786\n",
      "Iteration 480, loss = 0.44384907\n",
      "Iteration 481, loss = 0.44363389\n",
      "Iteration 482, loss = 0.44344365\n",
      "Iteration 483, loss = 0.44321567\n",
      "Iteration 484, loss = 0.44300226\n",
      "Iteration 485, loss = 0.44280294\n",
      "Iteration 486, loss = 0.44258097\n",
      "Iteration 487, loss = 0.44237953\n",
      "Iteration 488, loss = 0.44216959\n",
      "Iteration 489, loss = 0.44195877\n",
      "Iteration 490, loss = 0.44175088\n",
      "Iteration 491, loss = 0.44153527\n",
      "Iteration 492, loss = 0.44133468\n",
      "Iteration 493, loss = 0.44111885\n",
      "Iteration 494, loss = 0.44091973\n",
      "Iteration 495, loss = 0.44071897\n",
      "Iteration 496, loss = 0.44050076\n",
      "Iteration 497, loss = 0.44030582\n",
      "Iteration 498, loss = 0.44009707\n",
      "Iteration 499, loss = 0.43989184\n",
      "Iteration 500, loss = 0.43968825\n",
      "Iteration 501, loss = 0.43947705\n",
      "Iteration 502, loss = 0.43927527\n",
      "Iteration 503, loss = 0.43906950\n",
      "Iteration 504, loss = 0.43886320\n",
      "Iteration 505, loss = 0.43868650\n",
      "Iteration 506, loss = 0.43846714\n",
      "Iteration 507, loss = 0.43826491\n",
      "Iteration 508, loss = 0.43807383\n",
      "Iteration 509, loss = 0.43785718\n",
      "Iteration 510, loss = 0.43765959\n",
      "Iteration 511, loss = 0.43746288\n",
      "Iteration 512, loss = 0.43725950\n",
      "Iteration 513, loss = 0.43705925\n",
      "Iteration 514, loss = 0.43686132\n",
      "Iteration 515, loss = 0.43666621\n",
      "Iteration 516, loss = 0.43646258\n",
      "Iteration 517, loss = 0.43625816\n",
      "Iteration 518, loss = 0.43607059\n",
      "Iteration 519, loss = 0.43586426\n",
      "Iteration 520, loss = 0.43566878\n",
      "Iteration 521, loss = 0.43549038\n",
      "Iteration 522, loss = 0.43527675\n",
      "Iteration 523, loss = 0.43507950\n",
      "Iteration 524, loss = 0.43489912\n",
      "Iteration 525, loss = 0.43469075\n",
      "Iteration 526, loss = 0.43450246\n",
      "Iteration 527, loss = 0.43430331\n",
      "Iteration 528, loss = 0.43411290\n",
      "Iteration 529, loss = 0.43392308\n",
      "Iteration 530, loss = 0.43372580\n",
      "Iteration 531, loss = 0.43352883\n",
      "Iteration 532, loss = 0.43335131\n",
      "Iteration 533, loss = 0.43314193\n",
      "Iteration 534, loss = 0.43295385\n",
      "Iteration 535, loss = 0.43277980\n",
      "Iteration 536, loss = 0.43256714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 537, loss = 0.43239838\n",
      "Iteration 538, loss = 0.43219558\n",
      "Iteration 539, loss = 0.43200548\n",
      "Iteration 540, loss = 0.43182097\n",
      "Iteration 541, loss = 0.43162741\n",
      "Iteration 542, loss = 0.43144750\n",
      "Iteration 543, loss = 0.43124215\n",
      "Iteration 544, loss = 0.43106607\n",
      "Iteration 545, loss = 0.43086828\n",
      "Iteration 546, loss = 0.43069668\n",
      "Iteration 547, loss = 0.43049218\n",
      "Iteration 548, loss = 0.43030816\n",
      "Iteration 549, loss = 0.43012646\n",
      "Iteration 550, loss = 0.42993737\n",
      "Iteration 551, loss = 0.42974804\n",
      "Iteration 552, loss = 0.42958874\n",
      "Iteration 553, loss = 0.42938357\n",
      "Iteration 554, loss = 0.42920643\n",
      "Iteration 555, loss = 0.42902557\n",
      "Iteration 556, loss = 0.42883857\n",
      "Iteration 557, loss = 0.42864814\n",
      "Iteration 558, loss = 0.42847438\n",
      "Iteration 559, loss = 0.42828646\n",
      "Iteration 560, loss = 0.42810604\n",
      "Iteration 561, loss = 0.42792236\n",
      "Iteration 562, loss = 0.42773841\n",
      "Iteration 563, loss = 0.42756691\n",
      "Iteration 564, loss = 0.42737870\n",
      "Iteration 565, loss = 0.42719325\n",
      "Iteration 566, loss = 0.42701928\n",
      "Iteration 567, loss = 0.42685421\n",
      "Iteration 568, loss = 0.42665462\n",
      "Iteration 569, loss = 0.42648534\n",
      "Iteration 570, loss = 0.42630670\n",
      "Iteration 571, loss = 0.42612444\n",
      "Iteration 572, loss = 0.42593898\n",
      "Iteration 573, loss = 0.42577807\n",
      "Iteration 574, loss = 0.42558411\n",
      "Iteration 575, loss = 0.42541414\n",
      "Iteration 576, loss = 0.42523991\n",
      "Iteration 577, loss = 0.42506190\n",
      "Iteration 578, loss = 0.42488892\n",
      "Iteration 579, loss = 0.42471445\n",
      "Iteration 580, loss = 0.42454017\n",
      "Iteration 581, loss = 0.42436148\n",
      "Iteration 582, loss = 0.42419378\n",
      "Iteration 583, loss = 0.42402128\n",
      "Iteration 584, loss = 0.42384168\n",
      "Iteration 585, loss = 0.42367490\n",
      "Iteration 586, loss = 0.42349331\n",
      "Iteration 587, loss = 0.42332441\n",
      "Iteration 588, loss = 0.42316195\n",
      "Iteration 589, loss = 0.42297534\n",
      "Iteration 590, loss = 0.42280502\n",
      "Iteration 591, loss = 0.42265343\n",
      "Iteration 592, loss = 0.42246433\n",
      "Iteration 593, loss = 0.42228981\n",
      "Iteration 594, loss = 0.42212993\n",
      "Iteration 595, loss = 0.42195600\n",
      "Iteration 596, loss = 0.42178371\n",
      "Iteration 597, loss = 0.42161650\n",
      "Iteration 598, loss = 0.42146956\n",
      "Iteration 599, loss = 0.42128028\n",
      "Iteration 600, loss = 0.42110721\n",
      "Iteration 601, loss = 0.42097238\n",
      "Iteration 602, loss = 0.42078351\n",
      "Iteration 603, loss = 0.42062479\n",
      "Iteration 604, loss = 0.42045772\n",
      "Iteration 605, loss = 0.42029692\n",
      "Iteration 606, loss = 0.42012229\n",
      "Iteration 607, loss = 0.41995006\n",
      "Iteration 608, loss = 0.41979370\n",
      "Iteration 609, loss = 0.41962236\n",
      "Iteration 610, loss = 0.41946354\n",
      "Iteration 611, loss = 0.41929333\n",
      "Iteration 612, loss = 0.41913206\n",
      "Iteration 613, loss = 0.41896712\n",
      "Iteration 614, loss = 0.41880998\n",
      "Iteration 615, loss = 0.41864473\n",
      "Iteration 616, loss = 0.41848017\n",
      "Iteration 617, loss = 0.41832194\n",
      "Iteration 618, loss = 0.41815956\n",
      "Iteration 619, loss = 0.41799416\n",
      "Iteration 620, loss = 0.41782737\n",
      "Iteration 621, loss = 0.41769612\n",
      "Iteration 622, loss = 0.41751104\n",
      "Iteration 623, loss = 0.41736288\n",
      "Iteration 624, loss = 0.41720677\n",
      "Iteration 625, loss = 0.41704617\n",
      "Iteration 626, loss = 0.41688266\n",
      "Iteration 627, loss = 0.41671657\n",
      "Iteration 628, loss = 0.41656430\n",
      "Iteration 629, loss = 0.41641801\n",
      "Iteration 630, loss = 0.41624088\n",
      "Iteration 631, loss = 0.41609361\n",
      "Iteration 632, loss = 0.41593660\n",
      "Iteration 633, loss = 0.41577662\n",
      "Iteration 634, loss = 0.41563049\n",
      "Iteration 635, loss = 0.41546402\n",
      "Iteration 636, loss = 0.41531388\n",
      "Iteration 637, loss = 0.41515437\n",
      "Iteration 638, loss = 0.41500133\n",
      "Iteration 639, loss = 0.41485450\n",
      "Iteration 640, loss = 0.41469118\n",
      "Iteration 641, loss = 0.41454539\n",
      "Iteration 642, loss = 0.41438515\n",
      "Iteration 643, loss = 0.41423117\n",
      "Iteration 644, loss = 0.41408470\n",
      "Iteration 645, loss = 0.41393194\n",
      "Iteration 646, loss = 0.41377732\n",
      "Iteration 647, loss = 0.41362177\n",
      "Iteration 648, loss = 0.41348202\n",
      "Iteration 649, loss = 0.41332493\n",
      "Iteration 650, loss = 0.41317085\n",
      "Iteration 651, loss = 0.41301428\n",
      "Iteration 652, loss = 0.41288244\n",
      "Iteration 653, loss = 0.41271733\n",
      "Iteration 654, loss = 0.41256451\n",
      "Iteration 655, loss = 0.41243182\n",
      "Iteration 656, loss = 0.41227425\n",
      "Iteration 657, loss = 0.41212447\n",
      "Iteration 658, loss = 0.41197533\n",
      "Iteration 659, loss = 0.41184762\n",
      "Iteration 660, loss = 0.41168904\n",
      "Iteration 661, loss = 0.41153933\n",
      "Iteration 662, loss = 0.41138716\n",
      "Iteration 663, loss = 0.41125203\n",
      "Iteration 664, loss = 0.41111292\n",
      "Iteration 665, loss = 0.41094826\n",
      "Iteration 666, loss = 0.41082110\n",
      "Iteration 667, loss = 0.41067207\n",
      "Iteration 668, loss = 0.41052625\n",
      "Iteration 669, loss = 0.41037788\n",
      "Iteration 670, loss = 0.41024163\n",
      "Iteration 671, loss = 0.41008893\n",
      "Iteration 672, loss = 0.40994797\n",
      "Iteration 673, loss = 0.40980382\n",
      "Iteration 674, loss = 0.40966321\n",
      "Iteration 675, loss = 0.40952200\n",
      "Iteration 676, loss = 0.40937871\n",
      "Iteration 677, loss = 0.40924108\n",
      "Iteration 678, loss = 0.40909854\n",
      "Iteration 679, loss = 0.40895440\n",
      "Iteration 680, loss = 0.40881127\n",
      "Iteration 681, loss = 0.40867940\n",
      "Iteration 682, loss = 0.40853454\n",
      "Iteration 683, loss = 0.40839177\n",
      "Iteration 684, loss = 0.40825011\n",
      "Iteration 685, loss = 0.40813001\n",
      "Iteration 686, loss = 0.40797608\n",
      "Iteration 687, loss = 0.40783473\n",
      "Iteration 688, loss = 0.40769282\n",
      "Iteration 689, loss = 0.40758663\n",
      "Iteration 690, loss = 0.40742596\n",
      "Iteration 691, loss = 0.40728799\n",
      "Iteration 692, loss = 0.40715546\n",
      "Iteration 693, loss = 0.40702402\n",
      "Iteration 694, loss = 0.40688687\n",
      "Iteration 695, loss = 0.40674734\n",
      "Iteration 696, loss = 0.40660572\n",
      "Iteration 697, loss = 0.40648009\n",
      "Iteration 698, loss = 0.40634977\n",
      "Iteration 699, loss = 0.40619809\n",
      "Iteration 700, loss = 0.40607081\n",
      "Iteration 701, loss = 0.40595051\n",
      "Iteration 702, loss = 0.40580232\n",
      "Iteration 703, loss = 0.40566655\n",
      "Iteration 704, loss = 0.40553817\n",
      "Iteration 705, loss = 0.40541015\n",
      "Iteration 706, loss = 0.40530841\n",
      "Iteration 707, loss = 0.40520761\n",
      "Iteration 708, loss = 0.40509183\n",
      "Iteration 709, loss = 0.40498828\n",
      "Iteration 710, loss = 0.40485113\n",
      "Iteration 711, loss = 0.40474248\n",
      "Iteration 712, loss = 0.40463454\n",
      "Iteration 713, loss = 0.40452218\n",
      "Iteration 714, loss = 0.40440706\n",
      "Iteration 715, loss = 0.40430671\n",
      "Iteration 716, loss = 0.40419316\n",
      "Iteration 717, loss = 0.40407739\n",
      "Iteration 718, loss = 0.40396389\n",
      "Iteration 719, loss = 0.40388068\n",
      "Iteration 720, loss = 0.40375904\n",
      "Iteration 721, loss = 0.40365203\n",
      "Iteration 722, loss = 0.40357562\n",
      "Iteration 723, loss = 0.40343740\n",
      "Iteration 724, loss = 0.40335746\n",
      "Iteration 725, loss = 0.40324155\n",
      "Iteration 726, loss = 0.40313528\n",
      "Iteration 727, loss = 0.40302612\n",
      "Iteration 728, loss = 0.40295729\n",
      "Iteration 729, loss = 0.40282489\n",
      "Iteration 730, loss = 0.40271910\n",
      "Iteration 731, loss = 0.40264788\n",
      "Iteration 732, loss = 0.40251868\n",
      "Iteration 733, loss = 0.40242788\n",
      "Iteration 734, loss = 0.40235278\n",
      "Iteration 735, loss = 0.40224284\n",
      "Iteration 736, loss = 0.40214320\n",
      "Iteration 737, loss = 0.40204287\n",
      "Iteration 738, loss = 0.40194801\n",
      "Iteration 739, loss = 0.40184515\n",
      "Iteration 740, loss = 0.40175104\n",
      "Iteration 741, loss = 0.40165713\n",
      "Iteration 742, loss = 0.40157622\n",
      "Iteration 743, loss = 0.40146483\n",
      "Iteration 744, loss = 0.40138546\n",
      "Iteration 745, loss = 0.40127760\n",
      "Iteration 746, loss = 0.40119037\n",
      "Iteration 747, loss = 0.40108780\n",
      "Iteration 748, loss = 0.40099534\n",
      "Iteration 749, loss = 0.40091569\n",
      "Iteration 750, loss = 0.40081426\n",
      "Iteration 751, loss = 0.40072627\n",
      "Iteration 752, loss = 0.40066335\n",
      "Iteration 753, loss = 0.40055888\n",
      "Iteration 754, loss = 0.40045528\n",
      "Iteration 755, loss = 0.40037581\n",
      "Iteration 756, loss = 0.40031892\n",
      "Iteration 757, loss = 0.40019482\n",
      "Iteration 758, loss = 0.40011701\n",
      "Iteration 759, loss = 0.40003617\n",
      "Iteration 760, loss = 0.39993399\n",
      "Iteration 761, loss = 0.39985922\n",
      "Iteration 762, loss = 0.39977252\n",
      "Iteration 763, loss = 0.39970630\n",
      "Iteration 764, loss = 0.39961127\n",
      "Iteration 765, loss = 0.39952181\n",
      "Iteration 766, loss = 0.39943765\n",
      "Iteration 767, loss = 0.39936584\n",
      "Iteration 768, loss = 0.39927139\n",
      "Iteration 769, loss = 0.39918282\n",
      "Iteration 770, loss = 0.39911272\n",
      "Iteration 771, loss = 0.39902032\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(2, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1400, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[1, 0, 0, 1, 2, 2, 0, 1, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00518225, 0.99481775]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba([[1, 0, 0, 1, 2, 2, 0, 1, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[1, 1, 1, 1, 2, 1, 1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06008152, 0.93991848]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba([[1, 1, 1, 1, 2, 1, 1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
